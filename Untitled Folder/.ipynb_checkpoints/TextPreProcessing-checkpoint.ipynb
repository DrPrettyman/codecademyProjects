{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aeb0819",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e7b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ece63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edf4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "Sentence Tokenization:\n",
      "[\"An electrocardiogram is used to record the electrical \\n              conduction through a person's heart.\", 'The readings can \\n              be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "ecg_text = \"\"\"An electrocardiogram is used to record the electrical \n",
    "              conduction through a person\\'s heart. The readings can \n",
    "              be used to diagnose cardiac arrhythmias.\"\"\"\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "\n",
    "print('Word Tokenization:')\n",
    "print(tokenized_by_word)\n",
    "\n",
    "print('Sentence Tokenization:')\n",
    "print(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73beccb0",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3b8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased brands: salvation army, ymca, boys & girls club of america\n",
      "Uppercased brands: SALVATION ARMY, YMCA, BOYS & GIRLS CLUB OF AMERICA\n"
     ]
    }
   ],
   "source": [
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "brands_upper = brands.upper()\n",
    "\n",
    "print(f'Lowercased brands: {brands_lower}')\n",
    "print(f'Uppercased brands: {brands_upper}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118e2e3",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe38644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Tokenized: ['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n",
      "\n",
      "\n",
      "Text without Stops: ['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "# Save all English stopwords to the variable ``stop_words``\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "\n",
    "print(f'Words Tokenized: {tokenized_survey}')\n",
    "print('\\n')\n",
    "print(f'Text without Stops: {text_no_stops}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b24485",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "In natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits.\n",
    "\n",
    "NLTK has a built-in stemmer called PorterStemmer. You can use it with a list comprehension to stem each word in a tokenized list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972b9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you must import and initialize the stemmer:\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c946d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "populated_island = \"\"\"Java is an Indonesian island in the Pacific Ocean. \n",
    "                      It is the most populated island in the world,\n",
    "                      with over 140 million people.\"\"\"\n",
    "\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "stemmed = [stemmer.stem(word) for word in island_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02d670cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Tokenized:\n",
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Words Tokenized:')\n",
    "print(island_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e1dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words:\n",
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'it', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Stemmed Words:')\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfb5d5",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is a method for casting words to their root forms. This is a more involved process than stemming, because it requires the method to know the part-of-speech for each word. Since lemmatization requires the part of speech, it is a less efficient approach than stemming.\n",
    "\n",
    "In the next exercise, we will consider how to tag each word with a part of speech. In the meantime, let’s see how to use NLTK’s lemmatize operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "252c2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56ebf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in  tokenized_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de107d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Tokenized: ['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
      "Lemmatized Words: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Words Tokenized: {tokenized_string}')\n",
    "print(f'Lemmatized Words: {lemmatized_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b035aa",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "To improve the performance of lemmatization, we need to find the part of speech for each word in our string. In script.py, to the right, we created a part-of-speech tagging function. The function accepts a word, then returns the most common part of speech for that word. Let’s break down the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b13b647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joshua   :   n\n",
      "is   :   v\n",
      "a   :   n\n",
      "sexy   :   a\n",
      "beast   :   n\n",
      "and   :   n\n",
      "can   :   n\n",
      "dance   :   n\n",
      "like   :   v\n",
      "a   :   n\n",
      "feind   :   n\n",
      ".   :   n\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "# Just to see how this works:\n",
    "text = \"Joshua is a sexy beast and can dance like a feind.\"\n",
    "tokenized = word_tokenize(text)\n",
    "for word in tokenized:\n",
    "    print(word + '   :   ' + get_part_of_speech(word)) \n",
    "# So it interprets \"can\" and \"dance\" as nouns, but they are not nouns in this context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "467b17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_pos = [lemmatizer.lemmatize(word, get_part_of_speech(word)) for word in  tokenized_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd40c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words are: ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'The lemmatized words are: {lemmatized_pos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390352a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c1122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c00d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
