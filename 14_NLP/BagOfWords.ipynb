{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4347e6b",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58df1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf9f95",
   "metadata": {},
   "source": [
    "### Features Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65bd739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}\n"
     ]
    }
   ],
   "source": [
    "# Create the Features dictionary by assigning an index to every word in the training document\n",
    "\n",
    "def create_features_dictionary(documents):\n",
    "    features_dictionary = {}\n",
    "    merged = \" \".join(documents)\n",
    "    tokens = preprocess_text(merged)\n",
    "    index = 0\n",
    "    for token in tokens:\n",
    "        if token not in features_dictionary.keys():\n",
    "            features_dictionary[token] = index\n",
    "            index += 1\n",
    "    return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313b4dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Build a BoW vector\n",
    "\n",
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    tokens = preprocess_text(some_text)\n",
    "    for token in tokens:\n",
    "      index = features_dictionary[token]\n",
    "      bow_vector[index] += 1\n",
    "    return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2669815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's better to convert tokens to bow_vector\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    for token in document_tokens:\n",
    "      if token in features_dictionary:\n",
    "        feature_index = features_dictionary[token]\n",
    "        bow_vector[feature_index] += 1\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c8db2",
   "metadata": {},
   "source": [
    "### Classifying text\n",
    "\n",
    "We can use a Naive Bayes Classifier with our bags of words to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e85ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = ['I am in love with The Sun',\n",
    "            'Love, love, love music, life',\n",
    "            'All you need is love']\n",
    "training_docs = ['I am in love with music', \n",
    "                 'Life is Love', \n",
    "                 'Cats are terrible people',\n",
    "                 'love is the key to happiness', \n",
    "                 'The world is burning and I\\'m eating icecream',\n",
    "                 'I used to think that I should watch TV',\n",
    "                 'I used think that it was good for me',\n",
    "                 'We don\\'t need no education',\n",
    "                 'All you need is love']\n",
    "training_labels = [1, 1, 0, 1, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97a0ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_doc_tokens = [preprocess_text(text) for text in training_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "190b7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bow_sms_dictionary:\n",
    "bow_sms_dictionary = create_features_dictionary(training_docs)\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = [tokens_to_bow_vector(\n",
    "    training_doc, bow_sms_dictionary) \n",
    "                    for training_doc in training_docs]\n",
    "\n",
    "# Define test_vectors:\n",
    "test_vectors = [tokens_to_bow_vector(\n",
    "    test_doc, bow_sms_dictionary) for test_doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd37618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_vectors, training_labels)\n",
    "predictions = classifier.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43f4ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32451cc8",
   "metadata": {},
   "source": [
    "## Using libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa29f1e",
   "metadata": {},
   "source": [
    "Amazing work! As is the case with many tasks in Python, there’s already a library that can do all of that work for you.\n",
    "\n",
    "For ``text_to_bow()``, you can approximate the functionality with the ``collections`` module’s ``Counter()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd3429d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'another': 2, 'fish': 2, 'five': 1, 'find': 1, 'faraway': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    " \n",
    "tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
    "print(Counter(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b54bda",
   "metadata": {},
   "source": [
    "For vectorization, you can use ``CountVectorizer`` from the machine learning library ``scikit-learn``. You can use ``fit()`` to train the features dictionary and then ``transform()`` to transform text into a vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c791efa",
   "metadata": {},
   "source": [
    "Text preprocessing, tokenizing and filtering of stopwords are all included in ``sklearn.feature_extraction.CountVectorizer``, which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da8c14be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "test_text = [\"Another five fish find another faraway fish.\"]\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectorizer.fit(training_documents)\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "print(bow_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f774b2f",
   "metadata": {},
   "source": [
    "# Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e810bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "140ea5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "tokens = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45944bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three most frequent word sequences and the number of occurrences according to Bigrams:\n",
      "[(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Bigram approach:\n",
    "bigrams_prepped = ngrams(tokens, 2)\n",
    "bigrams = Counter(bigrams_prepped)\n",
    "print(\"Three most frequent word sequences and the number of occurrences according to Bigrams:\")\n",
    "print(bigrams.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62b249d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Three most frequent words and number of occurrences according to Bag-of-Words:\n",
      "[('fish', 4), ('fly', 3), ('day', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words approach:\n",
    "bag_of_words = Counter(tokens)\n",
    "most_common_three = bag_of_words.most_common(3)\n",
    "\n",
    "print(\"\\nThree most frequent words and number of occurrences according to Bag-of-Words:\")\n",
    "print(most_common_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd08de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77846c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e61da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b5009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
