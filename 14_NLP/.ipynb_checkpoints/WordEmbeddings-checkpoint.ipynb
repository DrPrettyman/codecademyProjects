{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0e817f",
   "metadata": {},
   "source": [
    "# Word Embeddings Are All About Distance\n",
    "\n",
    "The idea behind word embeddings is a theory known as the distributional hypothesis. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. With word embeddings, we map words that exist with the same context to similar places in our vector space (math-speak for the area in which our vectors exist).\n",
    "\n",
    "The numeric values that are assigned to the vector representation of a word are not important in their own right, but gather meaning from how similar or not words are to each other.\n",
    "\n",
    "Thus the cosine distance between words with similar contexts will be small, and the cosine distance between words that have very different contexts will be large.\n",
    "\n",
    "The literal values of a word’s embedding have no actual meaning. We gain value in word embeddings from comparing the different word vectors and seeing how similar or different they are. Encoded in these vectors, however, is latent information about how they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f2fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from modules.common_words import most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9148fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spaCy to find the vector corresponding to \n",
    "# each of the most common words\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "vectors = [nlp(_word).vector for _word in most_common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d78b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define find_closest_words\n",
    "def find_closest_words(word_list, vector_list, word_to_check):\n",
    "    return sorted(word_list, key=lambda x: cosine(\n",
    "        vector_list[word_list.index(word_to_check)], \n",
    "        vector_list[word_list.index(x)]\n",
    "        ))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d377544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'eat', 'dinner', 'fish', 'health', 'kitchen', 'good', 'animal', 'water', 'treat']\n"
     ]
    }
   ],
   "source": [
    "# find closest words to food\n",
    "close_to_food = find_closest_words(most_common_words, vectors, 'food')\n",
    "print(close_to_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08c3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summer', 'spring', 'fall', 'season', 'year', 'week', 'day', 'evening', 'during', 'last']\n"
     ]
    }
   ],
   "source": [
    "# find closest words to summer\n",
    "close_to_summer = find_closest_words(most_common_words, vectors, 'summer')\n",
    "print(close_to_summer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec151e",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "You might be asking yourself a question now. How did we arrive at the vector values that define a word vector? And how do we ensure that the values chosen place the vectors for words with similar context close together and the vectors for words with different usages far apart?\n",
    "\n",
    "Step in word2vec! Word2vec is a statistical learning algorithm that develops word embeddings from a corpus of text. Word2vec uses one of two different model architectures to come up with the values that define a collection of word embeddings.\n",
    "\n",
    "One method is to use the continuous bag-of-words (CBOW) representation of a piece of text. The word2vec model goes through each word in the training corpus, in order, and tries to predict what word comes at each position based on applying bag-of-words to the words that surround the word in question. In this approach, the order of the words does not matter!\n",
    "\n",
    "The other method word2vec can use to create word embeddings is continuous skip-grams. Skip-grams function similarly to n-grams, except instead of looking at groupings of n-consecutive words in a text, we can look at sequences of words that are separated by some specified distance between them.\n",
    "\n",
    "For example, consider the sentence ``\"The squids jump out of the suitcase\"``. The 1-skip-2-grams includes all the bigrams (2-grams) as well as the following subsequences:\n",
    "\n",
    "``(The, jump), (squids, out), (jump, of), (out, the), (of, suitcase)``\n",
    "\n",
    "When using continuous skip-grams, the order of context is taken into consideration! Because of this, the time it takes to train the word embeddings is slower than when using continuous bag-of-words. The results, however, are often much better!\n",
    "\n",
    "With either the continuous bag-of-words or continuous skip-grams representations as training data, word2vec then uses a shallow, 2-layer neural network to come up with the values that place words with a similar context in vectors near each other and words with different contexts in vectors far apart from each other.\n",
    "\n",
    "Let’s take a closer look to see how continuous bag-of-words and continuous skip-grams work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7d6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the worst of times.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = \"It was the best of times, it was the worst of times.\"\n",
    "print(sentence)\n",
    "\n",
    "# preprocessing\n",
    "sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8260843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set context_length\n",
    "context_length = 2\n",
    "\n",
    "# function to get cbows\n",
    "def get_cbows(sentence_lst, context_length):\n",
    "    cbows = list()\n",
    "    for i, val in enumerate(sentence_lst):\n",
    "        if i < context_length:\n",
    "            pass\n",
    "        elif i < len(sentence_lst) - context_length:\n",
    "            context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
    "            vectorizer = CountVectorizer()\n",
    "            vectorizer.fit_transform(context)\n",
    "            context_no_order = vectorizer.get_feature_names()\n",
    "            cbows.append((val,context_no_order))\n",
    "    return cbows\n",
    "\n",
    "# define cbows here:\n",
    "cbows = get_cbows(sentence_lst, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a12c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous Bag of Words\n",
      "('the', ['best', 'it', 'of', 'was'])\n",
      "('best', ['of', 'the', 'times', 'was'])\n",
      "('of', ['best', 'it', 'the', 'times'])\n",
      "('times,', ['best', 'it', 'of', 'was'])\n",
      "('it', ['of', 'the', 'times', 'was'])\n",
      "('was', ['it', 'the', 'times', 'worst'])\n",
      "('the', ['it', 'of', 'was', 'worst'])\n",
      "('worst', ['of', 'the', 'times', 'was'])\n"
     ]
    }
   ],
   "source": [
    "print('\\nContinuous Bag of Words')\n",
    "for cbow in cbows:\n",
    "    print(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b069c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get skip-grams\n",
    "def get_skip_grams(sentence_lst, context_length):\n",
    "    skip_grams = list()\n",
    "    for i, val in enumerate(sentence_lst):\n",
    "        if i < context_length:\n",
    "            pass\n",
    "        elif i < len(sentence_lst) - context_length:\n",
    "            context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
    "            skip_grams.append((val, context))\n",
    "    return skip_grams\n",
    "\n",
    "# define skip_grams here:\n",
    "skip_grams = get_skip_grams(sentence_lst, context_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8bd13e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skip Grams\n",
      "('the', ['it', 'was', 'best', 'of'])\n",
      "('best', ['was', 'the', 'of', 'times,'])\n",
      "('of', ['the', 'best', 'times,', 'it'])\n",
      "('times,', ['best', 'of', 'it', 'was'])\n",
      "('it', ['of', 'times,', 'was', 'the'])\n",
      "('was', ['times,', 'it', 'the', 'worst'])\n",
      "('the', ['it', 'was', 'worst', 'of'])\n",
      "('worst', ['was', 'the', 'of', 'times'])\n"
     ]
    }
   ],
   "source": [
    "print('\\nSkip Grams')\n",
    "for skip_gram in skip_grams:\n",
    "    print(skip_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56693016",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Depending on the corpus of text we select to train a word embedding model, different word embeddings will be created according to the context of the words in the given corpus. The larger and more generic a corpus, however, the more generalizable the word embeddings become.\n",
    "\n",
    "When we want to train our own word2vec model on a corpus of text, we can use the gensim package!\n",
    "\n",
    "In previous exercises, we have been using pre-trained word embedding models stored in spaCy. These models were trained, using word2vec, on blog posts and news articles collected by the Linguistic Data Consortium at the University of Pennsylvania. With gensim, however, we are able to build our own word embeddings on any corpus of text we like.\n",
    "\n",
    "To easily train a word2vec model on our own corpus of text, we can use gensim’s ``Word2Vec()`` function.\n",
    "\n",
    "``model = gensim.models.Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=2, sg=1)``\n",
    "\n",
    "- ``corpus`` is a list of lists, where each inner list is a document in the corpus and each element in the inner lists is a word token\n",
    "- ``vector_size`` determines how many dimensions our word embeddings will include. Word embeddings often have upwards of 1,000 dimensions! Here we will create vectors of 100-dimensions to keep things simple.\n",
    "- don’t worry about the rest of the keyword arguments here!\n",
    "\n",
    "To view the entire vocabulary used to train the word embedding model, we can use the ``.wv.vocab.items()`` method.\n",
    "\n",
    "``vocabulary_of_model = list(model.wv.vocab.items())``\n",
    "\n",
    "When we train a word2vec model on a smaller corpus of text, we pick up on the unique ways in which words of the text are used.\n",
    "\n",
    "For example, if we were using scripts from the television show Friends as a training corpus, the model would pick up on the unique ways in which words are used in the show. While the generalized vectors in a spaCy model might not place the vectors for “Ross” and “Rachel” close together, a gensim word embedding model trained on Friends’ scrips would place the vectors for words like “Ross” and “Rachel”, two characters that have a continuous on and off-again relationship throughout the show, very close together!\n",
    "\n",
    "To easily find which vectors gensim placed close together in its word embedding model, we can use the ``.wv.most_similar()`` method.\n",
    "\n",
    "``model.wv.most_similar(\"my_word_here\", topn=100)``\n",
    "\n",
    "- ``\"my_word_here\"`` is the target word token we want to find most similar words to\n",
    "- ``topn`` is a keyword argument that indicates how many similar word vectors we want returned\n",
    "\n",
    "One last gensim method we will explore is a rather fun one: ``.wv.doesnt_match()``.\n",
    "\n",
    "``model.wv.doesnt_match([\"asia\", \"mars\", \"pluto\"])``\n",
    "\n",
    "when given a list of terms in the vocabulary as an argument, ``.wv.doesnt_match()`` returns which term is furthest from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2dbe3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from modules.romeo_and_juliet import romeo_and_juliet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "608977ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tragedy', 'romeo', 'juliet', 'william', 'shakespeare', 'contents', 'prologue.', 'act', 'scene', 'i.', 'public', 'place.', 'scene', 'ii.', 'street.', 'scene', 'iii.', 'room', 'capulet’s', 'house.']\n"
     ]
    }
   ],
   "source": [
    "# load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# preprocess text\n",
    "romeo_and_juliet_processed = [[word for word in romeo_and_juliet.lower().split() if word not in stop_words]]\n",
    "\n",
    "# view inner list of romeo_and_juliet_processed\n",
    "print(romeo_and_juliet_processed[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db53d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word embeddings model of 100 dimensions\n",
    "model = gensim.models.Word2Vec(\n",
    "    romeo_and_juliet_processed, \n",
    "    vector_size=100, window=5, min_count=1, \n",
    "    workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d993555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('romeo.', 0.9969708323478699), ('good', 0.9966548681259155), ('juliet.', 0.9965584874153137), ('thou', 0.9963880777359009), ('nurse.', 0.9962818622589111), ('shall', 0.9961342811584473), ('like', 0.9960909485816956), ('thy', 0.9959562420845032), ('love', 0.9958600997924805), ('benvolio.', 0.9958415031433105), ('would', 0.9958189129829407), ('mercutio.', 0.9957923889160156), ('make', 0.9957923293113708), ('romeo,', 0.9956952929496765), ('may', 0.9956520199775696), ('come', 0.9954913258552551), ('o,', 0.9954451322555542), ('’tis', 0.9952890872955322), ('tell', 0.9952310919761658), ('yet', 0.9951685667037964)]\n"
     ]
    }
   ],
   "source": [
    "# similar to romeo\n",
    "similar_to_romeo = model.wv.most_similar('romeo', topn=20)\n",
    "print(similar_to_romeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1db2e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mercutio\n"
     ]
    }
   ],
   "source": [
    "# one is not like the others\n",
    "not_star_crossed_lover = model.wv.doesnt_match([\"romeo\", \"juliet\", \"mercutio\"])\n",
    "print(not_star_crossed_lover)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4e7ed",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "Lost in a multidimensional vector space after this lesson? We hope not! We have covered a lot here, so let’s take some time to recap.\n",
    "\n",
    "- Vectors are containers of information, and they can have anywhere from 1-dimension to hundreds or thousands of dimensions\n",
    "- Word embeddings are vector representations of a word, where words with similar contexts are represented with vectors that are closer together\n",
    "- spaCy is a package that enables us to view and use pre-trained word embedding models\n",
    "- The distance between vectors can be calculated in many ways, and the best way for measuring the distance between higher dimensional vectors is cosine distance\n",
    "- Word2Vec is a shallow neural network model that can build word embeddings using either continuous bag-of-words or continuous skip-grams\n",
    "- Gensim is a package that allows us to create and train word embedding models using any corpus of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c9b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ff985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
